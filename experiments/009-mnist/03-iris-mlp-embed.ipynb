{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris MLP\n",
    "\n",
    "Back 2 basix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from physics_mi.eval import plot_loss\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    EPOCHS = 100\n",
    "    LR = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset from sklearn\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "y_valid = torch.tensor(y_test, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, use_act=True, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        if use_act:\n",
    "            self.act = nn.ReLU()\n",
    "        self.use_act = use_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_act:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=16, output_dim=2, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            LinearLayer(input_dim, hidden_dim, use_act=True),\n",
    "            LinearLayer(hidden_dim, hidden_dim, use_act=True),\n",
    "            LinearLayer(hidden_dim, output_dim, use_act=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # cba messing with the dataset that's already neatly packaged\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "model = model = Net(input_dim=4, hidden_dim=64, output_dim=y_train.unique().shape[0])\n",
    "optimiser = Adam(model.parameters(), lr=config.LR, weight_decay=1e-2)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "total_steps = len(X_train) * config.EPOCHS\n",
    "scheduler = OneCycleLR(optimiser, max_lr=config.LR, total_steps=total_steps)\n",
    "\n",
    "log = []\n",
    "\n",
    "for epoch in tqdm(range(config.EPOCHS)):\n",
    "    log_sample = {}\n",
    "\n",
    "    model.train()\n",
    "    y_hat = model(X_train)\n",
    "    loss = loss_func(y_hat, y_train)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    optimiser.zero_grad()\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X_valid)\n",
    "        valid_loss = loss_func(y_hat, y_valid)\n",
    "\n",
    "    log_sample[\"valid_loss\"] = float(valid_loss)\n",
    "    log_sample[\"train_loss\"] = float(loss)\n",
    "    log.append(log_sample)\n",
    "\n",
    "log = pd.DataFrame(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(log[\"train_loss\"], log[\"valid_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, X_valid, y_valid):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(X_valid)\n",
    "\n",
    "    y_preds = out.numpy()\n",
    "    y_targs = y_valid.numpy()\n",
    "\n",
    "    return y_preds, y_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_targs = get_preds(model, X_valid, y_valid)\n",
    "y_preds = y_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_preds == y_targs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_targs, y_preds)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"g\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task independence\n",
    "\n",
    "I'm keen to produce some plots like I did in `007-comp-graph/005-soft-mixing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics_mi.analysis import capture_intermediate_outputs\n",
    "from physics_mi.graph import generate_graph\n",
    "from physics_mi.analysis import plot_similarity_matrix, get_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ios = capture_intermediate_outputs(model, X_valid)\n",
    "valid_ios[\"input\"] = X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldNet(Net):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.requires_grad_(False)\n",
    "\n",
    "    def forward(self, lidx, linputs):\n",
    "        sub_net = self.layers[lidx : lidx + 1]\n",
    "        out = sub_net(linputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "scaffold_model = ScaffoldNet(input_dim=4, hidden_dim=64, output_dim=y_train.unique().shape[0])\n",
    "scaffold_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ios = valid_ios\n",
    "layer_keys = [\n",
    "    \"input\",\n",
    "    \"layers.0.act\",\n",
    "    \"layers.1.act\",\n",
    "    \"layers.2.linear.bias\",\n",
    "]  # these are where from we would like to extract the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf, edf = generate_graph(ios, layer_keys, scaffold_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(layer_keys), figsize=(10, 10))\n",
    "\n",
    "for ax, key in zip(axes, layer_keys):\n",
    "    sims = get_sims(ndf, key)\n",
    "    plot_similarity_matrix(sims, ax=ax, axis=False, colorbar=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physics-mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
