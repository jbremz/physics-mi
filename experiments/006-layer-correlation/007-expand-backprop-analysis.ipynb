{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand backprop analysis\n",
    "\n",
    "- train multiple models for analysis\n",
    "- do backprop from final output to all of the intermediate layer outputs (as well as the inputs)\n",
    "- confirm task orthogonality throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from physics_mi.utils import set_all_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.randint(1, 2**32 - 1)\n",
    "# seed = 1322468781  # this one is very interesting\n",
    "set_all_seeds(seed)\n",
    "print(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Keeping this extremely simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, use_act=True, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        if use_act:\n",
    "            self.act = nn.ReLU()\n",
    "        self.use_act = use_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_act:\n",
    "            x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim=4, hidden_dim=16, output_dim=2, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layers = nn.Sequential(\n",
    "            LinearLayer(input_dim, hidden_dim, use_act=True),\n",
    "            LinearLayer(hidden_dim, hidden_dim, use_act=True),\n",
    "            LinearLayer(hidden_dim, output_dim, use_act=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 10000\n",
    "\n",
    "# Epsilon value\n",
    "eps = 0.5\n",
    "\n",
    "\n",
    "# Generate Y values\n",
    "def generate_Y(n_samples):\n",
    "    return torch.rand(n_samples)\n",
    "\n",
    "\n",
    "# Generate X values based on Y\n",
    "def generate_X(Y, eps):\n",
    "    X = torch.empty(len(Y), 2)\n",
    "    X[:, 0] = Y / (torch.rand(len(Y)) * (1 - eps) + eps)\n",
    "    X[:, 1] = Y / X[:, 0]\n",
    "\n",
    "    # Randomly swap x1 and x2\n",
    "    mask = torch.rand(len(Y)) < 0.5\n",
    "    swap_vals = X[:, 0][mask]\n",
    "    X[:, 0][mask] = X[:, 1][mask]\n",
    "    X[:, 1][mask] = swap_vals\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# Initial generation\n",
    "Y1 = generate_Y(n_samples)\n",
    "X1 = generate_X(Y1, eps)\n",
    "\n",
    "# Ensure they are statistically independent by generating new Y and X values\n",
    "Y2 = generate_Y(n_samples)\n",
    "X2 = generate_X(Y2, eps)\n",
    "\n",
    "# Stack X1 and X2 to get the desired shape\n",
    "X = torch.cat((X1, X2), dim=1)\n",
    "\n",
    "# Stack Y1 and Y2 for the desired shape\n",
    "Y = torch.stack((Y1, Y2), dim=1)\n",
    "\n",
    "# Validate the relationship\n",
    "assert torch.allclose(X[:, 0] * X[:, 1], Y[:, 0])\n",
    "assert torch.allclose(X[:, 2] * X[:, 3], Y[:, 1])\n",
    "\n",
    "# Print the shapes\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(X[:, 0], alpha=0.5, density=True, label=\"mass\")\n",
    "ax.hist(X[:, 1], alpha=0.5, density=True, label=\"acceleration\")\n",
    "ax.hist(Y[:, 0], alpha=0.5, density=True, label=\"force\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(X[:, 2], alpha=0.5, density=True, label=\"mass\")\n",
    "ax.hist(X[:, 3], alpha=0.5, density=True, label=\"acceleration\")\n",
    "ax.hist(Y[:, 1], alpha=0.5, density=True, label=\"force\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, both now look identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_inds = np.random.permutation(range(X.shape[0]))  # shuffled indices\n",
    "\n",
    "X_train = X[s_inds[:8000]]\n",
    "Y_train = Y[s_inds[:8000]]\n",
    "X_valid = X[s_inds[8000:]]\n",
    "Y_valid = Y[s_inds[8000:]]\n",
    "\n",
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I'll just do full gradient descent to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # number of epochs\n",
    "hidden_dim = 16  # number of hidden units\n",
    "\n",
    "model = Net(input_dim=4, hidden_dim=hidden_dim, output_dim=2)\n",
    "loss_func = nn.MSELoss()\n",
    "optimiser = Adam(model.parameters(), lr=1e-2)\n",
    "log = []\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    log_sample = {}\n",
    "\n",
    "    # Training update\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    Y_hat = model(X_train)\n",
    "    loss = loss_func(Y_hat, Y_train)\n",
    "    log_sample[\"train_loss\"] = float(loss.detach())\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    # Validation set\n",
    "    model.eval()\n",
    "    Y_hat = model(X_valid)\n",
    "    loss = loss_func(Y_hat, Y_valid)\n",
    "    log_sample[\"valid_loss\"] = float(loss.detach())\n",
    "\n",
    "    log.append(log_sample)\n",
    "\n",
    "df = pd.DataFrame(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics_mi.eval import *\n",
    "\n",
    "\n",
    "# need to avoid flattening here because we have multiple outputs\n",
    "def get_preds(model, X_valid, Y_valid):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(X_valid)\n",
    "\n",
    "    y_preds = out.numpy()\n",
    "    y_targs = Y_valid.numpy()\n",
    "\n",
    "    return y_preds, y_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(df[\"train_loss\"], df[\"valid_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_targs = get_preds(model, X_valid, Y_valid)\n",
    "y_preds.shape, y_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_valid_loss(model, loss_func, X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "plot_results(y_preds[:, 0], y_targs[:, 0], ax=axes[0])\n",
    "plot_results(y_preds[:, 1], y_targs[:, 1], ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tasks seem to be doing well in parallel ðŸ‘"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from physics_mi.analysis import capture_intermediate_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(N=100, vary_task=\"A\"):\n",
    "    pairs = np.concatenate(\n",
    "        np.stack(np.meshgrid(np.linspace(0, 1, N), np.linspace(0, 1, N))).T\n",
    "    )\n",
    "    pairs = torch.tensor(pairs, dtype=torch.float32)\n",
    "    if vary_task == \"A\":\n",
    "        inputs = torch.cat((pairs, torch.full((len(pairs), 2), 0.5)), dim=1)\n",
    "    if vary_task == \"B\":\n",
    "        inputs = torch.cat((torch.full((len(pairs), 2), 0.5), pairs), dim=1)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inputs = torch.cat(\n",
    "    (get_inputs(100, vary_task=\"A\"), get_inputs(100, vary_task=\"B\"))\n",
    ")\n",
    "task_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ios = capture_intermediate_outputs(model, X_valid)\n",
    "task_ios = capture_intermediate_outputs(model, task_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to compare the principal components at `layers.0.act` with those from `layers.1.act` I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcs(data):\n",
    "    mean = torch.mean(data, 0)\n",
    "    data_centered = data - mean\n",
    "\n",
    "    # Step 2: Compute the SVD\n",
    "    U, S, V = torch.svd(data_centered)\n",
    "\n",
    "    # The columns of V are the principal components\n",
    "    principal_components = V\n",
    "\n",
    "    # Step 3: Compute variances\n",
    "    variances = S.pow(2) / (data.size(0) - 1)\n",
    "\n",
    "    return principal_components, variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_acts = {}\n",
    "task_acts[\"0\"] = task_ios[\"layers.0.act\"]\n",
    "task_acts[\"1\"] = task_ios[\"layers.1.act\"]\n",
    "task_acts[\"0\"].shape, task_acts[\"1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acts = {}\n",
    "valid_acts[\"0\"] = valid_ios[\"layers.0.act\"]\n",
    "valid_acts[\"1\"] = valid_ios[\"layers.1.act\"]\n",
    "valid_acts[\"0\"].shape, valid_acts[\"1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pcs = {}\n",
    "valid_vars = {}\n",
    "valid_pcs[\"0\"], valid_vars[\"0\"] = get_pcs(valid_acts[\"0\"])\n",
    "valid_pcs[\"1\"], valid_vars[\"1\"] = get_pcs(valid_acts[\"1\"])\n",
    "valid_pcs[\"0\"].shape, valid_pcs[\"1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pc_acts(pcs, acts):\n",
    "    pc_acts = (pcs.T[None, :] * acts[:, None, :]).sum(-1)\n",
    "    return pc_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pc_acts = {}\n",
    "valid_pc_acts[\"0\"] = get_pc_acts(valid_pcs[\"0\"], valid_acts[\"0\"])\n",
    "valid_pc_acts[\"1\"] = get_pc_acts(valid_pcs[\"1\"], valid_acts[\"1\"])\n",
    "valid_pc_acts[\"0\"].shape, valid_pc_acts[\"1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ios[\"layers.0.act\"].shape, valid_ios[\"layers.1.act\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the scaffold net needs to allow me to backprop from the final output to every intermediate layer's activations (and the input as a sanity check). In this case that would be:\n",
    "- layer1\n",
    "- layer0\n",
    "- input\n",
    "\n",
    "Question is, do I engineer something elegant that extends to any depth architecture, or do I just do this manually for now? ðŸ¤”\n",
    "\n",
    "The elegant solution could include an argument to the forward method that selects the layer to inspect. There's a question of whether the input would be stored as it has been in the class or whether this time it would be simpler to store it externally (as there are now multiple inputs). I think the latter is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaffoldNet(Net):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.requires_grad_(False)\n",
    "\n",
    "    def forward(self, lidx, linputs):\n",
    "        sub_net = self.layers[lidx:]\n",
    "        out = sub_net(linputs)\n",
    "        return out\n",
    "\n",
    "\n",
    "scaffold_model = ScaffoldNet()\n",
    "scaffold_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = {}\n",
    "tasks = [\"A\", \"B\"]\n",
    "for i, task in enumerate(tasks):\n",
    "    linputs = valid_ios[\"layers.0.act\"].clone().requires_grad_(True)\n",
    "    out = scaffold_model(1, linputs)\n",
    "    loss = out[:, i].mean()\n",
    "    loss.backward()\n",
    "    grads[task] = {}\n",
    "    grads[task][\"gradients\"] = linputs.grad.clone().detach()\n",
    "\n",
    "for task, _ in grads.items():\n",
    "    grads[task][\"pcs\"], grads[task][\"vars\"] = get_pcs(grads[task][\"gradients\"])\n",
    "    uq_grads = grads[task][\"gradients\"].unique(dim=0)\n",
    "    uq_grads_norm = uq_grads.norm(dim=1)\n",
    "    grads[task][\"unique\"] = {}\n",
    "    grads[task][\"unique\"][\"comps\"] = uq_grads / uq_grads_norm[:, None]\n",
    "    grads[task][\"unique\"][\"norms\"] = uq_grads_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for ax, task in zip(axes, (\"A\", \"B\")):\n",
    "    ax.bar(range(1, 17), grads[task][\"vars\"])\n",
    "    ax.set_title(f\"Task {task}\")\n",
    "    ax.set_xlabel(\"PC\")\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "\n",
    "fig.suptitle(\"Task-wise principal components of activation gradients\")\n",
    "fig.set_tight_layout(\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is crude but it could be informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanA = grads[\"A\"][\"gradients\"].mean(0)\n",
    "meanB = grads[\"B\"][\"gradients\"].mean(0)\n",
    "meanA = meanA / meanA.norm()\n",
    "meanB = meanB / meanB.norm()\n",
    "\n",
    "torch.dot(meanA, meanB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = torch.einsum(\n",
    "    \"ij,kj->ik\", grads[\"A\"][\"unique\"][\"comps\"], grads[\"B\"][\"unique\"][\"comps\"]\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity(sims, title=\"Dot-product Similarity\", x_label=None):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    im = ax.imshow(sims, cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "\n",
    "    # I'm pretty sure this is the right way round from ij,ik->jk?\n",
    "    ax.set_xlabel(\"Task B PCA\", fontsize=14)\n",
    "    ax.set_ylabel(\"Task A PCA\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarity(sims, title=\"Dot-product Similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceA = slice(0, 10000)\n",
    "sliceB = slice(10000, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A PCs\n",
    "\n",
    "I'm first focusing on the gradient PCs extracted from backprop from the task A output and how much variance they explain in both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating how much variance is explained by these gradient PCs across both task datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idxs = grads[task][\"unique\"][\"norms\"].argsort(descending=True)\n",
    "norms = grads[task][\"unique\"][\"norms\"][sort_idxs]\n",
    "grad_acts = torch.einsum(\"ij,kj->ki\", grads[task][\"unique\"][\"comps\"], task_acts[\"0\"])[\n",
    "    :, sort_idxs\n",
    "]\n",
    "varA = grad_acts[sliceA].var(0)\n",
    "varB = grad_acts[sliceB].var(0)\n",
    "varA.shape, varB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_variances(varA, varB, grad_pc_variance, err_varA=None, err_varB=None):\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot the variances for Task A and Task B\n",
    "    ax1.bar(\n",
    "        range(len(varA)),\n",
    "        varA,\n",
    "        yerr=err_varA,\n",
    "        width=0.4,\n",
    "        align=\"center\",\n",
    "        label=\"Task A activations\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax1.bar(\n",
    "        range(len(varB)),\n",
    "        varB,\n",
    "        yerr=err_varB,\n",
    "        width=0.4,\n",
    "        align=\"center\",\n",
    "        label=\"Task B activations\",\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Unique Gradient Component\")\n",
    "    ax1.set_ylabel(\"Variance\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Create a second y-axis for PC importance\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(\n",
    "        range(len(grad_pc_variance)),\n",
    "        grad_pc_variance,\n",
    "        label=\"Gradient Component Norm\",\n",
    "        color=\"r\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    ax2.set_ylim(0)\n",
    "    ax2.set_ylabel(\"Norm\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    ax1.set_title(\n",
    "        \"Task-wise variance explained in activations by each unique gradient component\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_variances(varA, varB, norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idxs = grads[task][\"unique\"][\"norms\"].argsort(descending=True)\n",
    "norms = grads[task][\"unique\"][\"norms\"][sort_idxs]\n",
    "grad_acts = torch.einsum(\"ij,kj->ki\", grads[task][\"unique\"][\"comps\"], task_acts[\"0\"])[\n",
    "    :, sort_idxs\n",
    "]\n",
    "varA = grad_acts[sliceA].var(0)\n",
    "varB = grad_acts[sliceB].var(0)\n",
    "varA.shape, varB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_variances(varA, varB, norms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physics-mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
